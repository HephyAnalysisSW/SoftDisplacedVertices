{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07f001cf-6888-45ee-91f1-47a46c95ebe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU count:  76\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import neptune\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import uproot\n",
    "# import ROOT\n",
    "\n",
    "print('CPU count: ', torch.multiprocessing.cpu_count())\n",
    "\n",
    "# torch.set_num_threads(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c15bd7-e53c-41f3-b9b5-3ff0dae35e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_neptune = False\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c4bbc30-3ac8-44dc-b0c7-e97a389eaee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_neptune:\n",
    "    run = neptune.init_run(\n",
    "        project=\"alikaan.guven/ParT\",\n",
    "        api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhNDNjMWJhNS0wMDExLTQ2NzYtOWVjNS1lNzAzOWU4Mzc0MGMifQ==\",\n",
    "    )  # your credentials\n",
    "\n",
    "param = {\n",
    "    \"input_dim\": 5,\n",
    "    \"input_svdim\": 5,\n",
    "    \"num_classes\": 2,\n",
    "    \"pair_input_dim\": 4,\n",
    "    \"embed_dims\": [32, 128, 32],\n",
    "    \"for_inference\": True,\n",
    "    \"lr\": 1e-4\n",
    "\n",
    "}\n",
    "\n",
    "if use_neptune:\n",
    "    run[\"parameters\"] = stringify_unsupported(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe43b8c-0bb1-4daa-b179-4f5215d29c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = uproot.iterate((\"/scratch-cbe/users/alikaan.gueven/ML_KAAN/predict/stop_M1000_988_ct200_2018/outputs/out_NANOAODSIMoutput_2_Skim.root:Events\"), step_size=20)\n",
    "# x = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd9712f7-2ed6-49bb-a1cd-6fc223362c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x[\"Jet_pt\"][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13d0028e-75c3-4e2e-bdaa-68b5d19bd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ParT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda07b2a-046e-4b70-8670-8b7a1f0e0f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/groups/hephy/cms/alikaan.gueven/conda/envs/coffea_torch/lib/python3.11/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParticleTransformerDVTagger(\n",
      "  (trimmer): SequenceTrimmer()\n",
      "  (embed): Embed(\n",
      "    (input_bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (embed): Sequential(\n",
      "      (0): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=5, out_features=32, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (4): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (8): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (pair_embed): PairEmbed(\n",
      "    (embed): Sequential(\n",
      "      (0): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (1): Conv1d(4, 64, kernel_size=(1,), stride=(1,))\n",
      "      (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): GELU(approximate='none')\n",
      "      (4): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): GELU(approximate='none')\n",
      "      (7): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (8): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): GELU(approximate='none')\n",
      "      (10): Conv1d(64, 8, kernel_size=(1,), stride=(1,))\n",
      "      (11): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (blocks): ModuleList(\n",
      "    (0-7): 8 x Block(\n",
      "      (pre_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (post_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "      (pre_fc_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (act_dropout): Dropout(p=0.1, inplace=False)\n",
      "      (post_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (svembed): Embed(\n",
      "    (input_bn): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (embed): Sequential(\n",
      "      (0): LayerNorm((5,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): Linear(in_features=5, out_features=32, bias=True)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (4): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (5): GELU(approximate='none')\n",
      "      (6): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (8): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (cls_blocks): ModuleList(\n",
      "    (0-1): 2 x Block(\n",
      "      (pre_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): MultiheadAttention(\n",
      "        (out_proj): NonDynamicallyQuantizableLinear(in_features=32, out_features=32, bias=True)\n",
      "      )\n",
      "      (post_attn_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0, inplace=False)\n",
      "      (pre_fc_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (act): GELU(approximate='none')\n",
      "      (act_dropout): Dropout(p=0, inplace=False)\n",
      "      (post_fc_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc2): Linear(in_features=128, out_features=32, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "num parameters:  158138\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# model = ParT.ParticleTransformerTagger(pf_input_dim=5,\n",
    "#                                        sv_input_dim=5,\n",
    "#                                        num_classes=2,\n",
    "#                                        pair_input_dim=4,\n",
    "#                                        # embed_dims=[32, 128, 32],\n",
    "#                                        for_inference=True).to(device, dtype=float)\n",
    "\n",
    "model = ParT.ParticleTransformerDVTagger(input_dim=param['input_dim'],\n",
    "                                         input_svdim=param['input_svdim'],\n",
    "                                         num_classes=param['num_classes'],\n",
    "                                         pair_input_dim=param['pair_input_dim'],\n",
    "                                         embed_dims=param['embed_dims'],\n",
    "                                         for_inference=param['for_inference']).to(device, dtype=float)\n",
    "\n",
    "                                                              \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=param['lr'])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "num_parameters = sum(parameter.numel() for parameter in model.parameters())\n",
    "if use_neptune:\n",
    "    run['num_parameters'] = num_parameters\n",
    "\n",
    "print(model)\n",
    "print('num parameters: ', num_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d7a97e-7b9d-4c2e-9b4e-70a207dde3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MTUprootDataLoaderSimplified as MTUprootDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ded532db-4fb5-4385-a2c6-1dc8e898f7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ModifiedUprootIterator = MTUprootDataLoader.ModifiedUprootIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "796f3bd8-52af-4e79-9ee9-5552a4318974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize iterable dataset\n",
      "Initialize iterable dataset\n",
      "Initialize iterable dataset\n"
     ]
    }
   ],
   "source": [
    "MLDATADIR = '/scratch-cbe/users/alikaan.gueven/ML_KAAN'\n",
    "tmpList = glob.glob(f'{MLDATADIR}/predict/stop_M1000_*/**/*.root', recursive=True)\n",
    "random.shuffle(tmpList)\n",
    "tmpList = [file + ':Events' for file in tmpList]\n",
    "maxTrain = round(len(tmpList)*0.70)\n",
    "maxVal = round(len(tmpList)*0.80)\n",
    "trainList = tmpList[:maxTrain]\n",
    "valList = tmpList[maxTrain:maxVal]\n",
    "testList = tmpList[maxVal:]\n",
    "\n",
    "branchDict = {}\n",
    "branchDict['aux'] = ['nSDVSecVtx']\n",
    "branchDict['ev'] = ['MET_phi']\n",
    "branchDict['jet'] = ['Jet_phi', 'Jet_pt', 'Jet_eta']\n",
    "branchDict['sv'] = ['SDVSecVtx_Lxy', 'SDVSecVtx_LxySig', 'SDVSecVtx_pAngle', 'SDVSecVtx_charge', 'SDVSecVtx_ndof',\n",
    "                    'SDVIdxLUT_SecVtxIdx', 'SDVIdxLUT_TrackIdx']\n",
    "branchDict['tk'] = ['SDVTrack_pt', 'SDVTrack_dxy', 'SDVTrack_dxyError', 'SDVTrack_dz', 'SDVTrack_dzError',\n",
    "                    'SDVTrack_normalizedChi2', 'SDVTrack_eta', 'SDVTrack_phi']\n",
    "branchDict['label'] = ['SDVSecVtx_matchedLLPnDau_bydau']\n",
    "\n",
    "shuffle = True\n",
    "nWorkers = 8\n",
    "step_size = 256\n",
    "\n",
    "trainDataset = ModifiedUprootIterator(trainList, branchDict, shuffle=shuffle, nWorkers=nWorkers, step_size=step_size)\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, num_workers=nWorkers, prefetch_factor=1, persistent_workers= True)\n",
    "\n",
    "valDataset = ModifiedUprootIterator(valList, branchDict, shuffle=shuffle, nWorkers=nWorkers, step_size=step_size)\n",
    "valLoader = torch.utils.data.DataLoader(valDataset, num_workers=nWorkers, prefetch_factor=1, persistent_workers= True)\n",
    "\n",
    "testDataset = ModifiedUprootIterator(testList, branchDict, shuffle=shuffle, nWorkers=nWorkers, step_size=step_size)\n",
    "testLoader = torch.utils.data.DataLoader(testDataset, num_workers=nWorkers, prefetch_factor=1, persistent_workers= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb9002bf-e6cb-4a0e-a791-532739962708",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize iterable dataset\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'SDVSecVtx_matchedLLPnDau_bydau'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 48\u001b[0m\n\u001b[1;32m     34\u001b[0m tk_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVTrack_dxy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     35\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVTrack_dxyError\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     36\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVTrack_dz\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     37\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVTrack_dzError\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     38\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVTrack_normalizedChi2\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m sv_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVSecVtx_Lxy\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     41\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVSecVtx_LxySig\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     42\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVSecVtx_pAngle\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     43\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVSecVtx_charge\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m     44\u001b[0m                          X[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSDVSecVtx_ndof\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, np\u001b[38;5;241m.\u001b[39mnewaxis]\n\u001b[0;32m---> 48\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSDVSecVtx_matchedLLPnDau_bydau\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     49\u001b[0m y \u001b[38;5;241m=\u001b[39m label\n\u001b[1;32m     52\u001b[0m tk_pair_features \u001b[38;5;241m=\u001b[39m tk_pair_features\u001b[38;5;241m.\u001b[39mto(device, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'SDVSecVtx_matchedLLPnDau_bydau'"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    shuffle = False\n",
    "    nWorkers = 1\n",
    "    step_size = 100\n",
    "else:\n",
    "    shuffle = True\n",
    "    nWorkers = 8\n",
    "    step_size = 256\n",
    "\n",
    "\n",
    "trainDataset = ModifiedUprootIterator(trainList, branchDict, shuffle=shuffle, nWorkers=nWorkers, step_size=step_size)\n",
    "trainLoader = torch.utils.data.DataLoader(trainDataset, num_workers=nWorkers, prefetch_factor=1, persistent_workers= True)\n",
    "\n",
    "\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    losses = []\n",
    "    TP_batch = []\n",
    "    all_batch = []\n",
    "    \n",
    "    for batch_num, input_data in enumerate(trainLoader):\n",
    "        optimizer.zero_grad()\n",
    "        X = input_data\n",
    "\n",
    "        tk_pair_features = torch.cat((X['SDVTrack_px'][0],\n",
    "                                      X['SDVTrack_py'][0],\n",
    "                                      X['SDVTrack_pz'][0],\n",
    "                                      X['SDVTrack_E'][0],), dim=1)\n",
    "\n",
    "        tk_features = torch.cat((X['SDVTrack_dxy'][0],\n",
    "                                 X['SDVTrack_dxyError'][0],\n",
    "                                 X['SDVTrack_dz'][0],\n",
    "                                 X['SDVTrack_dzError'][0],\n",
    "                                 X['SDVTrack_normalizedChi2'][0]), dim=1)\n",
    "        \n",
    "        sv_features = torch.cat((X['SDVSecVtx_Lxy'][0],\n",
    "                                 X['SDVSecVtx_LxySig'][0],\n",
    "                                 X['SDVSecVtx_pAngle'][0],\n",
    "                                 X['SDVSecVtx_charge'][0],\n",
    "                                 X['SDVSecVtx_ndof'][0]), dim=1)[..., np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "        label = X['SDVSecVtx_matchedLLPnDau_bydau'][0]\n",
    "        y = label\n",
    "                \n",
    "        \n",
    "        tk_pair_features = tk_pair_features.to(device, dtype=float)\n",
    "        tk_features = tk_features.to(device, dtype=float)\n",
    "        sv_features = sv_features.to(device, dtype=float)\n",
    "        \n",
    "        y = y.to(device, dtype=float)\n",
    "\n",
    "        ymaxSig = torch.max(y > 1, axis=-1).values\n",
    "        ymaxSig = ymaxSig.float()\n",
    "        yBkg = (ymaxSig != 1).float()\n",
    "        y = torch.concatenate((yBkg[:, np.newaxis], ymaxSig[:, np.newaxis]), axis=-1)\n",
    "        \n",
    "        output = model(x=tk_features,\n",
    "                       v=tk_pair_features,\n",
    "                       x_sv=sv_features)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        optimizer.step()\n",
    "        # print('output: ', output.shape)\n",
    "        # print('y: ', y.shape)\n",
    "        # print(output>1)\n",
    "        # break\n",
    "        \n",
    "        output_mask = sv_features[:,0,0] == 0 # [batch_dim, Lxy, 1-dim]\n",
    "        acc = (torch.sum((output[~output_mask]>0.5) == y[~output_mask].data)) / (2*len(y[~output_mask])) # 2 is the number of classes\n",
    "        if use_neptune:\n",
    "            run[\"train/accuracy_batch\"].append(acc)\n",
    "            run[\"train/loss_batch\"].append(loss.item())\n",
    "\n",
    "        TP_batch.append((torch.sum(~output_mask) * acc).item())\n",
    "        all_batch.append(torch.sum(~output_mask).item())\n",
    "        \n",
    "        # if batch_num % 6 == 0:\n",
    "            # pass\n",
    "            # output_mask = sv_features[:,0,0] == 0 # [batch_dim, Lxy, 1-dim]\n",
    "            # acc = (torch.sum((output[~output_mask]>0.5) == y[~output_mask].data)) / (2*len(y[~output_mask])) # 2 is the number of classes\n",
    "            # run[\"accuracy\"].append(acc)\n",
    "            # run[\"loss\"].append(loss.item())\n",
    "            # if acc != 1.:\n",
    "                # print('\\tEpoch %d | Batch %d | Loss %6.2f | 1/(1-Accu) %6.2f' % (epoch, batch_num, loss.item(), 1/(1-acc)))\n",
    "                # print('\\tEpoch %d | Batch %d | Loss %6.2f | Acc %6.2f' % (epoch, batch_num, loss.item(), acc))\n",
    "            # else:\n",
    "                # print('\\tEpoch %d | Batch %d | Loss %6.2f | 1/(1-Accu) inf' % (epoch, batch_num, loss.item()))\n",
    "\n",
    "    if use_neptune:\n",
    "        run[\"train/accuracy_epoch\"].append(sum(TP_batch) / sum(all_batch))\n",
    "        run[\"train/losses_epoch\"].append(sum(losses)/len(losses))\n",
    "\n",
    "\n",
    "    \n",
    "    ###### Inference on TEST ######\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        TP_batch = []\n",
    "        all_batch = []\n",
    "        for batch_num, input_data in enumerate(valLoader):\n",
    "            X = input_data\n",
    "\n",
    "            tk_pair_features = torch.cat((X['SDVTrack_px'][0],\n",
    "                                          X['SDVTrack_py'][0],\n",
    "                                          X['SDVTrack_pz'][0],\n",
    "                                          X['SDVTrack_E'][0],), dim=1)\n",
    "    \n",
    "            tk_features = torch.cat((X['SDVTrack_dxy'][0],\n",
    "                                     X['SDVTrack_dxyError'][0],\n",
    "                                     X['SDVTrack_dz'][0],\n",
    "                                     X['SDVTrack_dzError'][0],\n",
    "                                     X['SDVTrack_normalizedChi2'][0]), dim=1)\n",
    "            \n",
    "            sv_features = torch.cat((X['SDVSecVtx_Lxy'][0],\n",
    "                                     X['SDVSecVtx_LxySig'][0],\n",
    "                                     X['SDVSecVtx_pAngle'][0],\n",
    "                                     X['SDVSecVtx_charge'][0],\n",
    "                                     X['SDVSecVtx_ndof'][0]), dim=1)[..., np.newaxis]\n",
    "    \n",
    "    \n",
    "    \n",
    "            label = X['SDVSecVtx_matchedLLPnDau_bydau'][0]\n",
    "            y = label\n",
    "    \n",
    "            tk_pair_features = tk_pair_features.to(device, dtype=float)\n",
    "            tk_features = tk_features.to(device, dtype=float)\n",
    "            sv_features = sv_features.to(device, dtype=float)\n",
    "            \n",
    "            y = y.to(device, dtype=float)\n",
    "    \n",
    "            ymaxSig = torch.max(y > 1, axis=-1).values\n",
    "            ymaxSig = ymaxSig.float()\n",
    "            yBkg = (ymaxSig != 1).float()\n",
    "            y = torch.concatenate((yBkg[:, np.newaxis], ymaxSig[:, np.newaxis]), axis=-1)\n",
    "            \n",
    "            output = model(x=tk_features,\n",
    "                           v=tk_pair_features,\n",
    "                           x_sv=sv_features)\n",
    "            loss = criterion(output, y)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            output_mask = sv_features[:,0,0] == 0 # [batch_dim, Lxy, 1-dim]\n",
    "            acc = (torch.sum((output[~output_mask]>0.5) == y[~output_mask].data)) / (2*len(y[~output_mask])) # 2 is the number of classes\n",
    "            if use_neptune:\n",
    "                run[\"val/accuracy_batch\"].append(acc)\n",
    "                run[\"val/loss_batch\"].append(loss.item())\n",
    "    \n",
    "            TP_batch.append((torch.sum(~output_mask) * acc).item())\n",
    "            all_batch.append(torch.sum(~output_mask).item())\n",
    "\n",
    "        if use_neptune:\n",
    "            run[\"val/accuracy_epoch\"].append(sum(TP_batch) / sum(all_batch))\n",
    "            run[\"val/losses_epoch\"].append(sum(losses)/len(losses))\n",
    "            \n",
    "    \n",
    "    \n",
    "    # if len(losses) > 0:\n",
    "    #     print('Epoch %d | Loss %6.2f' % (epoch, sum(losses)/len(losses)))\n",
    "    # else:\n",
    "    #     print('Epoch %d ' % (epoch))\n",
    "    end_time = time.time()\n",
    "    print(f\"Time spent for epoch: {end_time - start_time:.2f} s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968d291-5f71-42cc-a1e7-25f1edb1f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_neptune:\n",
    "    run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a4bc7-a78e-48c4-97db-34ec1371f913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df02f63f-2148-445e-8b2f-c0032dc0434a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7addded5-5343-461a-b113-8e507b13b058",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e2e2df-4f26-408c-9b9f-6c26fb278911",
   "metadata": {},
   "source": [
    "## Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8badf5de-8595-463b-9313-998673e3e327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# \n",
    "# correct_predictions = []\n",
    "# all_predictions = []\n",
    "# \n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     for (x,y) in testLoader:\n",
    "#         \n",
    "#         x = x.to(device, dtype=float)\n",
    "#         y = y.to(device, dtype=float)\n",
    "# \n",
    "#         x = x[0]\n",
    "# \n",
    "#         ymaxSig = torch.max(y[0] > 1, axis=-1).values\n",
    "#         ymaxSig = ymaxSig.float()\n",
    "#         yBkg = (ymaxSig != 1).float()\n",
    "#         y = torch.concatenate((yBkg, ymaxSig), axis=-1)\n",
    "#         \n",
    "#         pred = model(x).argmax(dim=-1)\n",
    "#         summ = np.sum(np.asarray(pred == y.argmax(dim=-1), dtype=float))\n",
    "#         size = pred.shape.numel()\n",
    "#         print(f'Batch accuracy after training: {summ/size:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623903b2-c0e9-4fc0-9197-242e0226dfed",
   "metadata": {},
   "source": [
    "## Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17506b02-e9c7-462c-86bf-5f21c91c116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# \n",
    "# with torch.no_grad():\n",
    "#     for (x,y) in trainLoader:\n",
    "#         \n",
    "#         x = x.to(device, dtype=float)\n",
    "#         y = y.to(device, dtype=float)\n",
    "# \n",
    "#         x = x[0]\n",
    "# \n",
    "#         ymaxSig = torch.max(y[0] > 1, axis=-1).values\n",
    "#         ymaxSig = ymaxSig.float()\n",
    "#         yBkg = (ymaxSig != 1).float()\n",
    "#         y = torch.concatenate((yBkg, ymaxSig), axis=-1)\n",
    "#         \n",
    "#         pred = model(x).argmax(dim=-1)\n",
    "#         summ = np.sum(np.asarray(pred == y.argmax(dim=-1), dtype=float))\n",
    "#         size = pred.shape.numel()\n",
    "#         print(f'Batch accuracy after training: {summ/size:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043635f-86f6-4dd5-b94a-16d5caf3d9f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0e0c5f-5f29-4f99-83e6-6e4bef770ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = model(x)\n",
    "# plt.hist(pred.detach().numpy()[:,1], bins=np.arange(0,1.01,0.01), histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.ylim(1,1e3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82148143-c3a2-4e14-a4b4-186ffc20328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.hist(y.detach().numpy()[:,1], bins=np.arange(0,1.01,0.01), histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.ylim(1,1e3)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ec0a79-089f-4fca-ae71-4bdee5fd9391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(y.argmax(dim=-1).numpy() == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74da469d-115a-40ad-a0c5-fb9b22d33ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(y.argmax(dim=-1).numpy() == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a31560-3917-4ff2-bb83-d4e7baad46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 438 / (346 + 438)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004c25a-ff12-4430-a9ea-95930aa8818d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
